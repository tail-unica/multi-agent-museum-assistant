{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T18:28:07.890741Z",
     "start_time": "2025-01-24T18:27:41.393212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from TTS.api import TTS\n",
    "import torch\n",
    "tts = TTS()\n",
    "# List available üê∏TTS models\n",
    "print(tts.list_models())\n",
    "# EN tts_models/en/ljspeech/glow-tts\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\",\n",
    "              config_path=\"/home/andrea/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2/config_mixed_fp8.json\",\n",
    "              model_path=\"/home/andrea/.local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2/\").to(\"cuda:0\")\n",
    "tts.synthesizer.tts_model = tts.synthesizer.tts_model.half().to(\"cuda:0\")\n"
   ],
   "id": "245e3a7752e692a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TTS.utils.manage.ModelManager object at 0x7f692dee0a60>\n",
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.speakers = torch.load(speaker_file_path)\n",
      "/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using model: xtts\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T18:28:40.623987Z",
     "start_time": "2025-01-24T18:28:40.444582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "745a3ce04021862",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20366"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T18:29:02.807360Z",
     "start_time": "2025-01-24T18:29:02.804845Z"
    }
   },
   "cell_type": "code",
   "source": "fr = \"Quel sujet intrigant ! En tant que guide local au mus√©e CIMA, je dois dire que nous n'avons pas d'expositions ou de pi√®ces sp√©cifiques directement li√©es aux pierres debouttes. Cependant, notre collection contient quelques pi√®ces fascinantes qui pourraient vous int√©resser. Par exemple, si vous allez dans la salle 3, vous trouverez une exposition impressionnante sur les civilisations anciennes, y compris le peuple nuragique qui a jadis habit√© la Sardaigne et d'autres parties de la M√©diterran√©e. Vous pourriez √™tre surpris d'apprendre que ces cultures primitives √©taient connues pour leurs structures en pierres √©labor√©es, qui parfois prenaient la forme de m√©galithes ou de pierres debouttes. Bien qu'on ne trouve pas de pi√®ces sp√©cifiques li√©es directement aux pierres debouttes, vous pouvez trouver des exemples intrigants d'architecture sardinaire ancienne dans notre exposition sur les √©tablissements nuragiques. Vous pourriez √™tre frapp√© par l'ing√©niosit√© et la ma√Ætrise qui allaient dans la construction de ces structures en pierre, qui souvent pr√©sentaient des designs complexes et des symboles. Si vous √™tes int√©ress√© √† apprendre plus sur le contexte culturel entourant ces structures en pierre anciennes, je vous recommande de consulter notre exposition sur les civilisations m√©diterran√©ennes (localis√©e dans la salle 2). Vous trouverez des pi√®ces fascinantes et des √©l√©ments d'information sur les vies des personnes qui ont v√©cu pendant cette p√©riode, y compris leurs pratiques spirituelles et langages symboliques. Qui sait ? Vous pourriez d√©couvrir des connections entre les pierres debouttes et d'autres traditions anciennes ! Enfin, si vous cherchez une exp√©rience plus interactive, je vous sugg√®re d'explorer nos expositions num√©riques ou de participer √† l'une de nos visites guid√©es (consultez le calendrier du mus√©e pour obtenir des informations d√©taill√©es). Nos membres du personnel connaissant sont toujours ravis de partager leur expertise et offrir des perspectives uniques sur les pi√®ces que nous avons en exposition. Maintenant, en revenant aux pierres debouttes ! Bien qu'on n'ait pas d'expositions sp√©cifiques sur ce sujet, je suis plus que pr√™t √† discuter avec vous du contexte plus large des cultures anciennes et de leur relation avec les structures en pierre.\"",
   "id": "76bcf5b315600f78",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T05:04:24.888007Z",
     "start_time": "2025-01-24T05:04:24.885911Z"
    }
   },
   "cell_type": "code",
   "source": "fr = \"Quel sujet intrigant ! En tant que guide local au mus√©e CIMA, je dois dire que nous n'avons pas d'expositions ou de pi√®ces sp√©cifiques directement li√©es aux pierres debouttes.\"",
   "id": "ca86bf2eb3e5e7df",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T05:06:00.412338Z",
     "start_time": "2025-01-24T05:06:00.410381Z"
    }
   },
   "cell_type": "code",
   "source": "it = \"Quali prove supportano la presenza di un insediamento ad Allai durante il periodo nuragico e la prima et√† del ferro?\"",
   "id": "e566a59bf32d666a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T05:07:34.927929Z",
     "start_time": "2025-01-24T05:07:34.925886Z"
    }
   },
   "cell_type": "code",
   "source": "de = \"Welche Belege gibt es f√ºr das Vorhandensein einer Siedlung in Allai w√§hrend der Nuraghen- und fr√ºhen Eisenzeit?\"",
   "id": "7e494225567ab814",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T17:35:12.035418Z",
     "start_time": "2025-01-24T17:34:58.906955Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6c897c2a056232dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.speakers = torch.load(speaker_file_path)\n",
      "/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T16:50:32.450670Z",
     "start_time": "2025-01-24T16:50:02.714230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the text into chunks\n",
    "chunks = fr.split(\". \")\n",
    "audio_segments = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    if chunk.strip():\n",
    "        audio = tts.tts(text=chunk, speaker_wav=\"Paris symbolise la c.m4a\", language=\"fr\")\n",
    "        audio_segments.append(audio)"
   ],
   "id": "385911052e72e945",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Quel sujet intrigant !', \"En tant que guide local au mus√©e CIMA, je dois dire que nous n'avons pas d'expositions ou de pi√®ces sp√©cifiques directement li√©es aux pierres debouttes\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 2.680145263671875\n",
      " > Real-time factor: 0.2590528258870671\n",
      " > Text splitted to sentences.\n",
      "['Cependant, notre collection contient quelques pi√®ces fascinantes qui pourraient vous int√©resser']\n",
      " > Processing time: 1.3142049312591553\n",
      " > Real-time factor: 0.23051274925436213\n",
      " > Text splitted to sentences.\n",
      "[\"Par exemple, si vous allez dans la salle 3, vous trouverez une exposition impressionnante sur les civilisations anciennes, y compris le peuple nuragique qui a jadis habit√© la Sardaigne et d'autres parties de la M√©diterran√©e\"]\n",
      " > Processing time: 2.9542019367218018\n",
      " > Real-time factor: 0.23026183722893123\n",
      " > Text splitted to sentences.\n",
      "[\"Vous pourriez √™tre surpris d'apprendre que ces cultures primitives √©taient connues pour leurs structures en pierres √©labor√©es, qui parfois prenaient la forme de m√©galithes ou de pierres debouttes\"]\n",
      " > Processing time: 2.4126646518707275\n",
      " > Real-time factor: 0.2413408923103249\n",
      " > Text splitted to sentences.\n",
      "[\"Bien qu'on ne trouve pas de pi√®ces sp√©cifiques li√©es directement aux pierres debouttes, vous pouvez trouver des exemples intrigants d'architecture sardinaire ancienne dans notre exposition sur les √©tablissements nuragiques\"]\n",
      " > Processing time: 2.952160596847534\n",
      " > Real-time factor: 0.23852062628425327\n",
      " > Text splitted to sentences.\n",
      "[\"Vous pourriez √™tre frapp√© par l'ing√©niosit√© et la ma√Ætrise qui allaient dans la construction de ces structures en pierre, qui souvent pr√©sentaient des designs complexes et des symboles\"]\n",
      " > Processing time: 2.2081027030944824\n",
      " > Real-time factor: 0.23135722175185003\n",
      " > Text splitted to sentences.\n",
      "['Si vous √™tes int√©ress√© √† apprendre plus sur le contexte culturel entourant ces structures en pierre anciennes, je vous recommande de consulter notre exposition sur les civilisations m√©diterran√©ennes (localis√©e dans la salle 2)']\n",
      " > Processing time: 2.894620418548584\n",
      " > Real-time factor: 0.23105408423470994\n",
      " > Text splitted to sentences.\n",
      "[\"Vous trouverez des pi√®ces fascinantes et des √©l√©ments d'information sur les vies des personnes qui ont v√©cu pendant cette p√©riode, y compris leurs pratiques spirituelles et langages symboliques\"]\n",
      " > Processing time: 2.336211681365967\n",
      " > Real-time factor: 0.23127589421611042\n",
      " > Text splitted to sentences.\n",
      "['Qui sait ?', \"Vous pourriez d√©couvrir des connections entre les pierres debouttes et d'autres traditions anciennes !\", \"Enfin, si vous cherchez une exp√©rience plus interactive, je vous sugg√®re d'explorer nos expositions num√©riques ou de participer √† l'une de nos visites guid√©es (consultez le calendrier du mus√©e pour obtenir des informations d√©taill√©es)\"]\n",
      " > Processing time: 4.749732971191406\n",
      " > Real-time factor: 0.22713229990364542\n",
      " > Text splitted to sentences.\n",
      "['Nos membres du personnel connaissant sont toujours ravis de partager leur expertise et offrir des perspectives uniques sur les pi√®ces que nous avons en exposition']\n",
      " > Processing time: 2.124892473220825\n",
      " > Real-time factor: 0.23107136744712772\n",
      " > Text splitted to sentences.\n",
      "['Maintenant, en revenant aux pierres debouttes !', \"Bien qu'on n'ait pas d'expositions sp√©cifiques sur ce sujet, je suis plus que pr√™t √† discuter avec vous du contexte plus large des cultures anciennes et de leur relation avec les structures en pierre.\"]\n",
      " > Processing time: 3.103783369064331\n",
      " > Real-time factor: 0.2269178490977072\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T18:29:06.970044Z",
     "start_time": "2025-01-24T18:29:06.396995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "wav = tts.tts_to_file(text=fr, speaker_wav=\"Paris symbolise la c.m4a\", language=\"fr\", file_path=\"ouptut.wav\")"
   ],
   "id": "6e0f4c1b32e9c3cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Quel sujet intrigant !', \"En tant que guide local au mus√©e CIMA, je dois dire que nous n'avons pas d'expositions ou de pi√®ces sp√©cifiques directement li√©es aux pierres debouttes.\", 'Cependant, notre collection contient quelques pi√®ces fascinantes qui pourraient vous int√©resser.', \"Par exemple, si vous allez dans la salle 3, vous trouverez une exposition impressionnante sur les civilisations anciennes, y compris le peuple nuragique qui a jadis habit√© la Sardaigne et d'autres parties de la M√©diterran√©e.\", \"Vous pourriez √™tre surpris d'apprendre que ces cultures primitives √©taient connues pour leurs structures en pierres √©labor√©es, qui parfois prenaient la forme de m√©galithes ou de pierres debouttes.\", \"Bien qu'on ne trouve pas de pi√®ces sp√©cifiques li√©es directement aux pierres debouttes, vous pouvez trouver des exemples intrigants d'architecture sardinaire ancienne dans notre exposition sur les √©tablissements nuragiques.\", \"Vous pourriez √™tre frapp√© par l'ing√©niosit√© et la ma√Ætrise qui allaient dans la construction de ces structures en pierre, qui souvent pr√©sentaient des designs complexes et des symboles.\", 'Si vous √™tes int√©ress√© √† apprendre plus sur le contexte culturel entourant ces structures en pierre anciennes, je vous recommande de consulter notre exposition sur les civilisations m√©diterran√©ennes (localis√©e dans la salle 2).', \"Vous trouverez des pi√®ces fascinantes et des √©l√©ments d'information sur les vies des personnes qui ont v√©cu pendant cette p√©riode, y compris leurs pratiques spirituelles et langages symboliques.\", 'Qui sait ?', \"Vous pourriez d√©couvrir des connections entre les pierres debouttes et d'autres traditions anciennes !\", \"Enfin, si vous cherchez une exp√©rience plus interactive, je vous sugg√®re d'explorer nos expositions num√©riques ou de participer √† l'une de nos visites guid√©es (consultez le calendrier du mus√©e pour obtenir des informations d√©taill√©es).\", 'Nos membres du personnel connaissant sont toujours ravis de partager leur expertise et offrir des perspectives uniques sur les pi√®ces que nous avons en exposition.', 'Maintenant, en revenant aux pierres debouttes !', \"Bien qu'on n'ait pas d'expositions sp√©cifiques sur ce sujet, je suis plus que pr√™t √† discuter avec vous du contexte plus large des cultures anciennes et de leur relation avec les structures en pierre.\"]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m wav \u001B[38;5;241m=\u001B[39m \u001B[43mtts\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtts_to_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspeaker_wav\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mParis symbolise la c.m4a\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mouptut.wav\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/api.py:334\u001B[0m, in \u001B[0;36mTTS.tts_to_file\u001B[0;34m(self, text, speaker, language, speaker_wav, emotion, speed, pipe_out, file_path, split_sentences, **kwargs)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Convert text to speech.\u001B[39;00m\n\u001B[1;32m    304\u001B[0m \n\u001B[1;32m    305\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;124;03m        Additional arguments for the model.\u001B[39;00m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_arguments(speaker\u001B[38;5;241m=\u001B[39mspeaker, language\u001B[38;5;241m=\u001B[39mlanguage, speaker_wav\u001B[38;5;241m=\u001B[39mspeaker_wav, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 334\u001B[0m wav \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    335\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    336\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspeaker\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspeaker\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    337\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlanguage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspeaker_wav\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspeaker_wav\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m    \u001B[49m\u001B[43msplit_sentences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_sentences\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msynthesizer\u001B[38;5;241m.\u001B[39msave_wav(wav\u001B[38;5;241m=\u001B[39mwav, path\u001B[38;5;241m=\u001B[39mfile_path, pipe_out\u001B[38;5;241m=\u001B[39mpipe_out)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m file_path\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/api.py:276\u001B[0m, in \u001B[0;36mTTS.tts\u001B[0;34m(self, text, speaker, language, speaker_wav, emotion, speed, split_sentences, **kwargs)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Convert text to speech.\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \n\u001B[1;32m    250\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;124;03m        Additional arguments for the model.\u001B[39;00m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_arguments(\n\u001B[1;32m    274\u001B[0m     speaker\u001B[38;5;241m=\u001B[39mspeaker, language\u001B[38;5;241m=\u001B[39mlanguage, speaker_wav\u001B[38;5;241m=\u001B[39mspeaker_wav, emotion\u001B[38;5;241m=\u001B[39memotion, speed\u001B[38;5;241m=\u001B[39mspeed, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    275\u001B[0m )\n\u001B[0;32m--> 276\u001B[0m wav \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msynthesizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtts\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspeaker_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspeaker\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlanguage_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlanguage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspeaker_wav\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspeaker_wav\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    281\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreference_wav\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstyle_wav\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstyle_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreference_speaker_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m    \u001B[49m\u001B[43msplit_sentences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_sentences\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wav\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/utils/synthesizer.py:386\u001B[0m, in \u001B[0;36mSynthesizer.tts\u001B[0;34m(self, text, speaker_name, language_name, speaker_wav, style_wav, style_text, reference_wav, reference_speaker_name, split_sentences, **kwargs)\u001B[0m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sen \u001B[38;5;129;01min\u001B[39;00m sens:\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtts_model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msynthesize\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 386\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtts_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msynthesize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msen\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtts_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    389\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspeaker_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspeaker_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvoice_dirs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvoice_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m            \u001B[49m\u001B[43md_vector\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspeaker_embedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspeaker_wav\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspeaker_wav\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlanguage_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    396\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    397\u001B[0m         \u001B[38;5;66;03m# synthesize voice\u001B[39;00m\n\u001B[1;32m    398\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m synthesis(\n\u001B[1;32m    399\u001B[0m             model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtts_model,\n\u001B[1;32m    400\u001B[0m             text\u001B[38;5;241m=\u001B[39msen,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    408\u001B[0m             language_id\u001B[38;5;241m=\u001B[39mlanguage_id,\n\u001B[1;32m    409\u001B[0m         )\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/tts/models/xtts.py:419\u001B[0m, in \u001B[0;36mXtts.synthesize\u001B[0;34m(self, text, config, speaker_wav, language, speaker_id, **kwargs)\u001B[0m\n\u001B[1;32m    412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minference(text, language, gpt_cond_latent, speaker_embedding, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msettings)\n\u001B[1;32m    413\u001B[0m settings\u001B[38;5;241m.\u001B[39mupdate({\n\u001B[1;32m    414\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt_cond_len\u001B[39m\u001B[38;5;124m\"\u001B[39m: config\u001B[38;5;241m.\u001B[39mgpt_cond_len,\n\u001B[1;32m    415\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt_cond_chunk_len\u001B[39m\u001B[38;5;124m\"\u001B[39m: config\u001B[38;5;241m.\u001B[39mgpt_cond_chunk_len,\n\u001B[1;32m    416\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_ref_len\u001B[39m\u001B[38;5;124m\"\u001B[39m: config\u001B[38;5;241m.\u001B[39mmax_ref_len,\n\u001B[1;32m    417\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msound_norm_refs\u001B[39m\u001B[38;5;124m\"\u001B[39m: config\u001B[38;5;241m.\u001B[39msound_norm_refs,\n\u001B[1;32m    418\u001B[0m })\n\u001B[0;32m--> 419\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfull_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspeaker_wav\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msettings\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/tts/models/xtts.py:480\u001B[0m, in \u001B[0;36mXtts.full_inference\u001B[0;34m(self, text, ref_audio_path, language, temperature, length_penalty, repetition_penalty, top_k, top_p, do_sample, gpt_cond_len, gpt_cond_chunk_len, max_ref_len, sound_norm_refs, **hf_generate_kwargs)\u001B[0m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39minference_mode()\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfull_inference\u001B[39m(\n\u001B[1;32m    423\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhf_generate_kwargs,\n\u001B[1;32m    440\u001B[0m ):\n\u001B[1;32m    441\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    442\u001B[0m \u001B[38;5;124;03m    This function produces an audio clip of the given text being spoken with the given reference voice.\u001B[39;00m\n\u001B[1;32m    443\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;124;03m        Sample rate is 24kHz.\u001B[39;00m\n\u001B[1;32m    479\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 480\u001B[0m     (gpt_cond_latent, speaker_embedding) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_conditioning_latents\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    481\u001B[0m \u001B[43m        \u001B[49m\u001B[43maudio_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mref_audio_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgpt_cond_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgpt_cond_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgpt_cond_chunk_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgpt_cond_chunk_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_ref_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_ref_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    485\u001B[0m \u001B[43m        \u001B[49m\u001B[43msound_norm_refs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msound_norm_refs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    486\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minference(\n\u001B[1;32m    489\u001B[0m         text,\n\u001B[1;32m    490\u001B[0m         language,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    499\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhf_generate_kwargs,\n\u001B[1;32m    500\u001B[0m     )\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/tts/models/xtts.py:365\u001B[0m, in \u001B[0;36mXtts.get_conditioning_latents\u001B[0;34m(self, audio_path, max_ref_length, gpt_cond_len, gpt_cond_chunk_len, librosa_trim_db, sound_norm_refs, load_sr)\u001B[0m\n\u001B[1;32m    362\u001B[0m     audio \u001B[38;5;241m=\u001B[39m librosa\u001B[38;5;241m.\u001B[39meffects\u001B[38;5;241m.\u001B[39mtrim(audio, top_db\u001B[38;5;241m=\u001B[39mlibrosa_trim_db)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    364\u001B[0m \u001B[38;5;66;03m# compute latents for the decoder\u001B[39;00m\n\u001B[0;32m--> 365\u001B[0m speaker_embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_speaker_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mload_sr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    366\u001B[0m speaker_embeddings\u001B[38;5;241m.\u001B[39mappend(speaker_embedding)\n\u001B[1;32m    368\u001B[0m audios\u001B[38;5;241m.\u001B[39mappend(audio)\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/tts/models/xtts.py:320\u001B[0m, in \u001B[0;36mXtts.get_speaker_embedding\u001B[0;34m(self, audio, sr)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39minference_mode()\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_speaker_embedding\u001B[39m(\u001B[38;5;28mself\u001B[39m, audio, sr):\n\u001B[1;32m    318\u001B[0m     audio_16k \u001B[38;5;241m=\u001B[39m torchaudio\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mresample(audio, sr, \u001B[38;5;241m16000\u001B[39m)\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 320\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhifigan_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspeaker_encoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio_16k\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml2_norm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m         \u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    322\u001B[0m         \u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    323\u001B[0m     )\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/tts/layers/xtts/hifigan_decoder.py:538\u001B[0m, in \u001B[0;36mResNetSpeakerEncoder.forward\u001B[0;34m(self, x, l2_norm)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;66;03m# if you torch spec compute it otherwise use the mel spec computed by the AP\u001B[39;00m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_torch_spec:\n\u001B[0;32m--> 538\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtorch_spec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_input:\n\u001B[1;32m    541\u001B[0m     x \u001B[38;5;241m=\u001B[39m (x \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1e-6\u001B[39m)\u001B[38;5;241m.\u001B[39mlog()\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/media/Workspace/Projects/multi-agent-museum-assistant/mama-venv/lib/python3.10/site-packages/TTS/tts/layers/xtts/hifigan_decoder.py:418\u001B[0m, in \u001B[0;36mPreEmphasis.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x\u001B[38;5;241m.\u001B[39msize()) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m    417\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mpad(x\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m), (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreflect\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 418\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilter\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-24T19:03:05.695509Z",
     "start_time": "2025-01-24T19:02:59.556222Z"
    }
   },
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from agents.retriever import Retriever\n",
    "from agents.profiling import get_user_info\n",
    "from agents.generation import Generator\n",
    "from config import config as cfg\n",
    "from orchestrator import Orchestrator\n",
    "model = Ollama(model=cfg.model_local)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/tmp/ipykernel_4000161/791528125.py:7: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=cfg.model_local)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T19:03:09.685582Z",
     "start_time": "2025-01-24T19:03:09.683255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "del model\n",
    "model = Ollama(model=cfg.model_local)"
   ],
   "id": "dfc6217c28ed98d5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f083abbbe9626c48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:50:37.418008Z",
     "start_time": "2024-12-11T23:50:08.935834Z"
    }
   },
   "cell_type": "code",
   "source": "orchestrator = Orchestrator()",
   "id": "6f3d1c89c434fb94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/Workspace/Projects/multi-agent-museum-assistant/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/Workspace/Projects/multi-agent-museum-assistant/src/agents/retriever.py:49: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding=embeddings.OllamaEmbeddings(model=embedding_model)\n",
      "/media/Workspace/Projects/multi-agent-museum-assistant/src/agents/generation.py:13: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  self.model_local = Ollama(model=model_local)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:50:37.554789Z",
     "start_time": "2024-12-11T23:50:37.552644Z"
    }
   },
   "cell_type": "code",
   "source": "question = \"Tell me something about yourself\"",
   "id": "46d8cfca1de1d3d7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T00:02:22.287527Z",
     "start_time": "2024-12-11T23:51:01.160077Z"
    }
   },
   "cell_type": "code",
   "source": "orchestrator.generator.get_answer(question)",
   "id": "f6d72ed8f4fa47b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_contextvars.Context object at 0x7f5dd4585140>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't know. The provided context is a collection of documents with no personal information about myself.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T00:03:41.018152Z",
     "start_time": "2024-12-12T00:03:40.829802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open(\"orchestrator.bin\", 'wb') as f:\n",
    "    pickle.dump(orchestrator,\n",
    "                f,\n",
    "                protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "id": "a528ffe9f7f9d6b0",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'weakref.ReferenceType' object",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morchestrator.bin\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m----> 3\u001B[0m     \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdump\u001B[49m\u001B[43m(\u001B[49m\u001B[43morchestrator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHIGHEST_PROTOCOL\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: cannot pickle 'weakref.ReferenceType' object"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T03:06:29.357625Z",
     "start_time": "2024-12-12T03:06:29.244433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()"
   ],
   "id": "854f5506aa574bb6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T03:07:43.729679Z",
     "start_time": "2024-12-12T03:07:43.727153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chroma_db_config = {\n",
    "    \"client\": \"PersistentClient\",\n",
    "    \"timeout\": 20,\n",
    "    \"path\": \"../tmp/chroma\"\n",
    "}"
   ],
   "id": "d4ee8edfb45f52d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T03:12:11.614794Z",
     "start_time": "2024-12-12T03:12:11.606686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "storage_path = \"../tmp/chroma\"\n",
    "if storage_path is None:\n",
    "    raise ValueError('STORAGE_PATH environment variable is not set')\n",
    "\n",
    "client = chromadb.PersistentClient(path=storage_path)\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"test\")"
   ],
   "id": "10451ff762dfe02f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T03:25:58.458035Z",
     "start_time": "2024-12-12T03:25:58.455636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = \"\"\"Answer the question based only on the following context:\n",
    "        {context}. If the question does not match the context, just say that you don't know.  \n",
    "        Question: {question}\n",
    "        \"\"\""
   ],
   "id": "ec0647529dc40fbc",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T03:58:26.873063Z",
     "start_time": "2025-01-26T03:58:26.870091Z"
    }
   },
   "cell_type": "code",
   "source": "text = \"<think> Okay, I'm trying to figure out how to help this user who's asking about Joe Biden. They've given a detailed explanation of his life and career, which is quite extensive. Now, they're directing me to a museum scenario where I need to suggest other operas inside it based on the context provided. First, let me make sure I understand the task correctly. The user provided both a general question about Biden and then an in-depth look at his life. But their main instruction is about a museum setting, specifically suggesting opera exhibits related to the context they provided. Looking back at the initial context, it's about an art exhibit featuring American history with various artists and styles. There are specific pieces mentioned: Crossing the Delaware by E.E. Cummings, The Last Bird Home by Langston Hughes, Phantom of the Operations by Winslow B thrill, and Operation Clear Skies by Bruce na√Øve. Each piece represents a different era in American history. So, I need to think about how to suggest other operas or related art pieces that could be in such an exhibit. But wait, the initial context doesn't mention anything about operas directly. It's focused on visual art and historical themes through different artists' styles. So maybe the user is trying to connect Biden's background with opera themes. Given that Biden has a long political career, perhaps suggesting operas that reflect his influence or themes like leadership, history, etc. The exhibit already has four pieces covering specific historical periods: 1792 (Shays' Rebellion), 1865 (Emancipation of Slaves), World War II, and the Cold War. So, to expand this, I could add more operas or art works that highlight themes like American identity, cultural contributions, or personal struggles. For example, something from the Renaissance period might show the triumph of individualism, or a Baroque piece could reflect the complexity of human emotions. Impressionist works might symbolize freedom and innovation. I should also consider how each addition ties back to the broader theme of American history and culture. Each new work would represent a different era or aspect that complements the existing exhibit. This way, the visitor can see how various periods in American history are depicted through art, much like Biden's political journey spans multiple decades. Additionally, I should mention the location within the museum where each piece would be placed to give the user a clear idea of the layout. For example, placing Renaissance works in one section and Baroque pieces in another could create a chronological flow from earlier to more recent art periods. Finally, I need to make sure that my suggestions are only among the ones mentioned in the provided context, which seems to be limited to specific historical pieces already present. So adding other art forms or pieces that fit within this framework would help enrich the exhibit without deviating from the given context. </think> In addition to the existing works that reflect significant moments in American history and culture, we could consider adding other art pieces that highlight key aspects of American identity and history, complementing the current display. Here are some suggestions: 1. Renaissance Art: Works like The Family of Man by Leonardo da Vinci or Raphael's School of Athens to symbolize humanism and individual achievement during a time of cultural revival. 2. Baroque Period: Pieces such as Bach's Cantata or Rembrandt's Portraits to depict the dramatic and complex emotions prevalent in this era, reflecting both personal struggles and societal changes. 3. Impressionist Paintings: Works like Gauguin's At the Sea to symbolize freedom, innovation, and the influence of nature on art, capturing a period of cultural transformation. Each of these pieces would be placed strategically within the museum to create a narrative thread that aligns with the broader theme of American history, much like Biden's journey through various periods in his political career. This thematic approach would allow visitors to explore different eras and aspects of American culture, enriching their understanding of both historical contexts and individual struggles.\"",
   "id": "e0e550784e49b92e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T03:58:28.184744Z",
     "start_time": "2025-01-26T03:58:28.182395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Regular expression to remove everything between <think> and </think>\n",
    "cleaned_text = re.sub(r\"<think>.*?</think>\", \"\", text)\n",
    "\n",
    "# Remove extra spaces (optional)\n",
    "cleaned_text = \" \".join(cleaned_text.split())"
   ],
   "id": "b0f15d9e60f8ebc6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T03:58:30.931998Z",
     "start_time": "2025-01-26T03:58:30.929986Z"
    }
   },
   "cell_type": "code",
   "source": "text = \" \".join(cleaned_text.split())",
   "id": "1d497b63071ad0be",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T03:58:40.934535Z",
     "start_time": "2025-01-26T03:58:39.388887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Create a Translator object\n",
    "translator = Translator()\n",
    "\n",
    "# Text to translate\n",
    "#text = \"Hello, how are you?\"\n",
    "\n",
    "# Translate the text\n",
    "translation = translator.translate(text, src=\"en\", dest=\"it\")\n",
    "\n",
    "# Print the translation\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Translated: {translation.text}\")\n"
   ],
   "id": "276e4c231b710ca5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: In addition to the existing works that reflect significant moments in American history and culture, we could consider adding other art pieces that highlight key aspects of American identity and history, complementing the current display. Here are some suggestions: 1. Renaissance Art: Works like The Family of Man by Leonardo da Vinci or Raphael's School of Athens to symbolize humanism and individual achievement during a time of cultural revival. 2. Baroque Period: Pieces such as Bach's Cantata or Rembrandt's Portraits to depict the dramatic and complex emotions prevalent in this era, reflecting both personal struggles and societal changes. 3. Impressionist Paintings: Works like Gauguin's At the Sea to symbolize freedom, innovation, and the influence of nature on art, capturing a period of cultural transformation. Each of these pieces would be placed strategically within the museum to create a narrative thread that aligns with the broader theme of American history, much like Biden's journey through various periods in his political career. This thematic approach would allow visitors to explore different eras and aspects of American culture, enriching their understanding of both historical contexts and individual struggles.\n",
      "Translated: Oltre alle opere esistenti che riflettono momenti significativi nella storia e nella cultura americana, potremmo prendere in considerazione l'aggiunta di altre opere d'arte che evidenziano gli aspetti chiave dell'identit√† e della storia americani, completando l'attuale display.Ecco alcuni suggerimenti: 1. Arte rinascimentale: opere come la famiglia di uomo di Leonardo da Vinci o la scuola di Atene di Raphael per simboleggiare l'umanesimo e i risultati individuali durante un periodo di risveglio culturale.2. Periodo barocco: pezzi come la cantata di Bach o i ritratti di Rembrandt per rappresentare le emozioni drammatiche e complesse prevalenti in questa era, riflettendo sia le lotte personali che i cambiamenti sociali.3. Dipinti impressionisti: opere come Gauguin's At The Sea per simboleggiare la libert√†, l'innovazione e l'influenza della natura sull'arte, catturando un periodo di trasformazione culturale.Ognuno di questi pezzi verrebbe posizionato strategicamente all'interno del museo per creare un filo narrativo che si allinea al tema pi√π ampio della storia americana, proprio come il viaggio di Biden attraverso vari periodi della sua carriera politica.Questo approccio tematico consentirebbe ai visitatori di esplorare diverse epoche e aspetti della cultura americana, arricchendo la loro comprensione sia dei contesti storici che delle lotte individuali.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T04:23:45.055413Z",
     "start_time": "2025-01-26T04:23:44.161051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gtts import gTTS\n",
    "\n",
    "speak = gTTS(text=\"Oltre alle opere esistenti che riflettono momenti significativi nella storia e nella cultura americana, potremmo prendere in considerazione l'aggiunta di altre opere d'arte\", lang=\"it\", slow=False)\n",
    "speak.save(\"text_to_speech.mp3\")"
   ],
   "id": "c2ca056a36f7e49f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T05:03:16.170363Z",
     "start_time": "2025-01-26T05:03:15.886579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for id , mic in enumerate(spr.Microphone.list_microphone_names()):\n",
    "    print(f\"ID: {id}, Microphone: {mic}\")\n"
   ],
   "id": "e01f1e2f2725c622",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0, Microphone: HDA NVidia: HDMI 0 (hw:0,3)\n",
      "ID: 1, Microphone: HDA NVidia: HDMI 1 (hw:0,7)\n",
      "ID: 2, Microphone: HDA NVidia: HDMI 2 (hw:0,8)\n",
      "ID: 3, Microphone: HDA NVidia: HDMI 3 (hw:0,9)\n",
      "ID: 4, Microphone: HDA NVidia: HDMI 4 (hw:0,10)\n",
      "ID: 5, Microphone: HDA NVidia: HDMI 5 (hw:0,11)\n",
      "ID: 6, Microphone: HDA NVidia: HDMI 6 (hw:0,12)\n",
      "ID: 7, Microphone: HD-Audio Generic: ALCS1200A Analog (hw:1,0)\n",
      "ID: 8, Microphone: HD-Audio Generic: ALCS1200A Digital (hw:1,1)\n",
      "ID: 9, Microphone: HD-Audio Generic: ALCS1200A Alt Analog (hw:1,2)\n",
      "ID: 10, Microphone: webcam: USB Audio (hw:2,0)\n",
      "ID: 11, Microphone: USB PnP Audio Device: Audio (hw:3,0)\n",
      "ID: 12, Microphone: hdmi\n",
      "ID: 13, Microphone: pulse\n",
      "ID: 14, Microphone: default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm_dsnoop.c:601:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T05:06:01.917405Z",
     "start_time": "2025-01-26T05:05:57.562905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import speech_recognition as spr\n",
    "\n",
    "\n",
    "recog1 = spr.Recognizer()\n",
    "mc = spr.Microphone(11)\n",
    "\n",
    "with mc as source:\n",
    "   recog1.adjust_for_ambient_noise(source, duration=0.9)\n",
    "   audio = recog1.listen(source, phrase_time_limit=5)\n",
    "get_sentence = recog1.recognize_google(audio)"
   ],
   "id": "626780855dde2c49",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm_dsnoop.c:601:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm_dsnoop.c:601:(snd_pcm_dsnoop_open) unable to open slave\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:397:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:482:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib pcm_dmix.c:1032:(snd_pcm_dmix_open) unable to open slave\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
